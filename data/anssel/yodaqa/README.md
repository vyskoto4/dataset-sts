Answer Sentence Selection (YodaQA)
==================================

In Question Answering systems on top of unstructured corpora, one task is
selecting the sentences in corpora that are most likely to carry an answer
to a given question.  In this scenario, the questions come from the
``curated-train`` YodaQA dataset and the YodaQA system generated the
candidate sentences based on enwiki, using YodaQA f/sentence-selection
branch commit a5f7f98 (based on v1.5).

Sentences were generated by running fulltext solr search on enwiki for
keywords extracted from the question, and then considering all sentences
from top N results that contain at least a single such keyword.  Sentences
that match the gold standard answer regex are labelled as 1, the rest is 0.
This *automatic labelling* means the dataset is **quite noisy**.

The key metric here is MRR when ranking sentences answering the same question
by their score, but raw accuracy may be also interesting.  The dataset is
heavily unbalanced!  In our models, we subsample 0-entries as well as
supersample 1-entries.  First 80% entries result in the training set, last 20%
is validation set.  The test set is generated from the ``curated-test`` set.

This dataset is subject to change and evolution, hence the ``v2``.

Moreover, an extended dataset which is probably even more noisy (as the
questions haven't been so carefully curated, and gold standard regexes not
so carefully reviewed either) but about 3-4 times bigger is available as
``large2470``.  The splits of this dataset are supersets of the ``curated``
dataset.

TODO: Figure out whether it's better to report final results on the large2470
or curated test split.

Model Comparison
----------------

For randomized models, 95% confidence intervals (t-distribution) are reported.

curatedv2:

| Model                    | trainAllMRR | devMRR   | testMAP  | testMRR  | settings
|--------------------------|-------------|----------|----------|----------|---------
| termfreq TF-IDF #w       | 0.339544    | 0.324693 | 0.242700 | 0.337893 | ``freq_mode='tf'``
| termfreq BM25 #w         | 0.483538    | 0.452647 | 0.294300 | 0.484530 | (defaults)
|--------------------------|-------------|----------|----------|----------|---------
| avg                      | 0.422881    | 0.402618 | 0.229694 | 0.329356 | (defaults)
|                          |±0.024685    |±0.006664 |±0.001715 |±0.003511 |
| DAN                      | 0.437119    | 0.430754 | 0.233000 | 0.354075 | ``inp_e_dropout=0`` ``inp_w_dropout=1/3`` ``deep=2`` ``pact='relu'``
|                          |±0.014494    |±0.014477 |±0.002657 |±0.010307 |
|--------------------------|-------------|----------|----------|----------|---------
| rnn                      | 0.459869    | 0.429780 | 0.228869 | 0.341706 | (defaults)
|                          |±0.035981    |±0.015609 |±0.005554 |±0.010643 |
| cnn                      | 0.544067    | 0.363028 | 0.228538 | 0.309165 | (defaults)
|                          |±0.037730    |±0.011041 |±0.004791 |±0.009649 |
| rnncnn                   | 0.578608    | 0.374195 | 0.238200 | 0.344659 | (defaults)
|                          |±0.044228    |±0.023533 |±0.007741 |±0.014747 |
| attn1511                 | 0.432403    | 0.475125 | 0.275219 | 0.468555 | (defaults)
|                          |±0.016183    |±0.012810 |±0.006562 |±0.014433 |

large2470:

| Model                    | trainAllMRR | devMRR   | testMAP  | testMRR  | settings
|--------------------------|-------------|----------|----------|----------|---------
| termfreq TF-IDF #w       | 0.325390    | 0.328255 | 0.266800 | 0.362613 | ``freq_mode='tf'``
| termfreq BM25 #w         | 0.441573    | 0.432115 | 0.313900 | 0.490822 | (defaults)
|--------------------------|-------------|----------|----------|----------|---------
| avg                      | 0.798883    | 0.408034 | 0.262569 | 0.362190 | (defaults)
|                          |±0.026554    |±0.004656 |±0.002054 |±0.005725 |
| DAN                      | 0.646481    | 0.404210 | 0.272675 | 0.386522 | ``inp_e_dropout=0`` ``inp_w_dropout=1/3`` ``deep=2`` ``pact='relu'``
|                          |±0.070994    |±0.005378 |±0.003028 |±0.007627 |
|--------------------------|-------------|----------|----------|----------|---------
| rnn                      | 0.460984    | 0.382949 | 0.262463 | 0.381298 | (defaults)
|                          |±0.023715    |±0.006451 |±0.002641 |±0.007643 | 
| attn1511                 | 0.445635    | 0.408495 | 0.288100 | 0.430892 | (defaults)
|                          |±0.056352    |±0.008744 |±0.005601 |±0.017858 |

Older Datasets
--------------

The academic standard up to now stems from **anssel/wang** - the TREC-based
dataset originally by Wang et al., 2007, in the form by Yao et al., 2013.
However, this dataset seems to be quite easy as the ratio of relevant snippets
is high and they are often pretty short.

That's why we are introducing a new one, which is also somewhat bigger.

See also the WikiQA corpus, which has too restrictive licence but manually
labelled pairs.

Licence
-------

Derived from the ``curated-train`` YodaQA dataset

	htts://github.com/brmson/dataset-factoid-curated

and sentences from Wikipedia, Wikipedia is CC-BY-SA if that's relevant.
